{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "tweets_treino_masterchefbrasil.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNQgjhujZ4Ez"
      },
      "source": [
        "# **Trabalho Pr√°tico 1: Atividade em conjunto de classifica√ß√£o de texto**\n",
        "### Grupo: Bianca Muniz, Renan Cavalcante, Sara Magalh√£es e Talita Burbulhan\n",
        "### Tema: Masterchef Brasil\n",
        "**Objetivo**: Efetuar a classifica√ß√£o do corpus capturado na primeira aula e classes determinadas pelos grupos\n",
        "\n",
        "**Passos**\n",
        "* Criar corpus de treino\n",
        "* Rodar algoritmos de treino e classifica√ß√£o\n",
        "* Enviar 1 link para notebook no google colab por grupo"
      ],
      "id": "qNQgjhujZ4Ez"
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "93328deb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fd68dd4-e617-4111-b043-939003eb5725"
      },
      "source": [
        "# Instalando e importando bibliotecas\n",
        "!pip3 install snscrape\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "import pandas as pd"
      ],
      "id": "93328deb",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting snscrape\n",
            "  Downloading snscrape-0.3.4-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from snscrape) (4.2.6)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from snscrape) (4.6.3)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from snscrape) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->snscrape) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->snscrape) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->snscrape) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->snscrape) (2021.5.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->snscrape) (1.7.1)\n",
            "Installing collected packages: snscrape\n",
            "Successfully installed snscrape-0.3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b4937e8",
        "scrolled": false
      },
      "source": [
        "# Criando uma lista para coletar os tweets com a query - biblioteca snscrape\n",
        "tweets = []\n",
        "# Pegando mil tweets em portugu√™s da hashtag #masterchefbrasil\n",
        "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('#masterchefbrasil lang:pt -filter:replies').get_items()):\n",
        "    if i>1000:\n",
        "        break\n",
        "    tweets.append([tweet.id, tweet.date, tweet.content, tweet.username])\n",
        "\n",
        "masterchef = pd.DataFrame(tweets)\n",
        "masterchef.to_csv(\"masterchef_treino.csv\", sep='\\t', encoding='utf-8', index=False)"
      ],
      "id": "5b4937e8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXmjcrlqVv2g"
      },
      "source": [
        "Importamos o arquivo CSV para o Google Drive e classificamos os tweets manualmente em tr√™s grupos (neutro, positivo ou negativo), at√© cada grupo ter 210 tweets. A planilha pode ser vista no [link](https://github.com/biamuniz/mineracao_masterchefbrasil/blob/main/tweets_treino.csv)."
      ],
      "id": "nXmjcrlqVv2g"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taRkLgi5Vf0h"
      },
      "source": [
        "#Abrindo o arquivo CSV com os tweets classificados\n",
        "dados = pd.read_csv(r'https://raw.githubusercontent.com/biamuniz/mineracao_masterchefbrasil/main/dados/tweets_treino.csv', sep=',')"
      ],
      "id": "taRkLgi5Vf0h",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "zLddg7TNXJdj",
        "outputId": "a90684ce-2549-4bb7-dfe8-2b3dc507d75c"
      },
      "source": [
        "dados"
      ],
      "id": "zLddg7TNXJdj",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>data</th>\n",
              "      <th>tweet</th>\n",
              "      <th>user</th>\n",
              "      <th>classifica√ß√£o</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1456834542627602433</td>\n",
              "      <td>2021-11-06 04:03:16+00:00</td>\n",
              "      <td>O que aconteceu com o Foga√ßa? Vendo agora o ep...</td>\n",
              "      <td>sferreiravic</td>\n",
              "      <td>Neutro</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1456765784529768458</td>\n",
              "      <td>2021-11-05 23:30:03+00:00</td>\n",
              "      <td>Em 2020, Mar√≠lia participou do epis√≥dio especi...</td>\n",
              "      <td>RevistaISTOE</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1456764222977761284</td>\n",
              "      <td>2021-11-05 23:23:50+00:00</td>\n",
              "      <td>Maria Rita e Jo√£o Carlos Martins s√£o os convid...</td>\n",
              "      <td>tvabordo</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1456442462743015452</td>\n",
              "      <td>2021-11-05 02:05:17+00:00</td>\n",
              "      <td>Eu n√£o entendo a persegui√ß√£o com o Heitor e o ...</td>\n",
              "      <td>sabilisacrf</td>\n",
              "      <td>Positivo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1456316523019128840</td>\n",
              "      <td>2021-11-04 17:44:50+00:00</td>\n",
              "      <td>O hor√°rio de trabalho de todo mundo que trabal...</td>\n",
              "      <td>chefanahirata</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>1418040353115066378</td>\n",
              "      <td>2021-07-22 02:49:00+00:00</td>\n",
              "      <td>Gente‚Ä¶fa√ßo nem ideia de quem seja essa duda be...</td>\n",
              "      <td>nnalourenco</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>1418040340305620999</td>\n",
              "      <td>2021-07-22 02:48:57+00:00</td>\n",
              "      <td>Eu comecei n√£o gostando da Amanda e agora ela ...</td>\n",
              "      <td>ccolemanholland</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>1418040289365827592</td>\n",
              "      <td>2021-07-22 02:48:45+00:00</td>\n",
              "      <td>Algu√©m sabe quem √© Duda Beat?\\n#masterchefbrasil</td>\n",
              "      <td>LuizMelloc</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>1418040242339209218</td>\n",
              "      <td>2021-07-22 02:48:34+00:00</td>\n",
              "      <td>Duda Beat no #masterchefbrasil √© tudo pra mim ü•∞ü•∞</td>\n",
              "      <td>lholhohelo</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1000</th>\n",
              "      <td>1418040232692404229</td>\n",
              "      <td>2021-07-22 02:48:32+00:00</td>\n",
              "      <td>Essa Juliana tem uma cara de enjoada... \\n#mas...</td>\n",
              "      <td>ravennacomcats</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1001 rows √ó 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                       id  ... classifica√ß√£o\n",
              "0     1456834542627602433  ...        Neutro\n",
              "1     1456765784529768458  ...           NaN\n",
              "2     1456764222977761284  ...           NaN\n",
              "3     1456442462743015452  ...      Positivo\n",
              "4     1456316523019128840  ...           NaN\n",
              "...                   ...  ...           ...\n",
              "996   1418040353115066378  ...           NaN\n",
              "997   1418040340305620999  ...           NaN\n",
              "998   1418040289365827592  ...           NaN\n",
              "999   1418040242339209218  ...           NaN\n",
              "1000  1418040232692404229  ...           NaN\n",
              "\n",
              "[1001 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjNk58FhaUZg"
      },
      "source": [
        "### **Filtrando os tweets classificados e salvando cada grupo em um arquivo de texto diferente**"
      ],
      "id": "VjNk58FhaUZg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6P3FSBGXN82"
      },
      "source": [
        "Tweets negativos"
      ],
      "id": "e6P3FSBGXN82"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIlaijnOVoAI"
      },
      "source": [
        "negativo = dados.loc[dados['classifica√ß√£o'] == 'Negativo'] ## filtrando os tweets com classifica√ß√£o \"Negativo\" e armazenando no objeto \"negativo\"\n",
        "tweet_negativo = negativo.drop(columns=['id', \"data\", \"user\", \"classifica√ß√£o\"]) ##excluindo colunas e deixando s√≥ a coluna de interesse (tweet)\n",
        "tweet_negativo.to_csv(\"tweets_negativo.txt\", sep='\\t', encoding='utf-8', index = False) ## salvando o objeto em txt com o nome tweets_negativo.txt"
      ],
      "id": "YIlaijnOVoAI",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_xX0fqQXpB8"
      },
      "source": [
        "Tweets positivos"
      ],
      "id": "g_xX0fqQXpB8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jz5id5ZXXW4P"
      },
      "source": [
        "positivo = dados.loc[dados['classifica√ß√£o'] == 'Positivo'] ## filtrando os tweets com classifica√ß√£o \"Positivo\" e armazenando no objeto \"negativo\"\n",
        "tweet_positivo = positivo.drop(columns=['id', \"data\", \"user\", \"classifica√ß√£o\"]) ##excluindo colunas e deixando s√≥ a coluna de interesse (tweet)\n",
        "tweet_positivo.to_csv(\"tweets_positivo.txt\", sep='\\t', encoding='utf-8', index = False) ## salvando o objeto em txt com o nome tweets_negativo.txt"
      ],
      "id": "Jz5id5ZXXW4P",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgYVHoznXrDB"
      },
      "source": [
        "Tweets neutros"
      ],
      "id": "QgYVHoznXrDB"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVOs7vykXWpk"
      },
      "source": [
        "neutro = dados.loc[dados['classifica√ß√£o'] == 'Neutro'] ## filtrando os tweets com classifica√ß√£o \"Negativo\" e armazenando no objeto \"negativo\"\n",
        "tweet_neutro = neutro.drop(columns=['id', \"data\", \"user\", \"classifica√ß√£o\"]) ##excluindo colunas e deixando s√≥ a coluna de interesse (tweet)\n",
        "tweet_neutro.to_csv(\"tweets_neutro.txt\", sep='\\t', encoding='utf-8', index = False) ## salvando o objeto em txt com o nome tweets_negativo.txt"
      ],
      "id": "OVOs7vykXWpk",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOeAH7pIaiqa"
      },
      "source": [
        "### **Rodando o modelo**"
      ],
      "id": "KOeAH7pIaiqa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7CvyVIFXxw5",
        "outputId": "56651ddc-4704-485d-f09b-9ca8ef3c806b"
      },
      "source": [
        "!pip install nltk"
      ],
      "id": "I7CvyVIFXxw5",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6J-GkO0X4SL"
      },
      "source": [
        "from nltk.classify import NaiveBayesClassifier, MaxentClassifier, SklearnClassifier\n",
        "import csv\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "def divide(dados):    \n",
        "    dados_new = []\n",
        "    for palavra in dados:\n",
        "        palavra_filter = [i.lower() for i in palavra.split()]\n",
        "        dados_new.append(palavra_filter)\n",
        "    return dados_new\n",
        "\n",
        "def bag_of_words(palavras):\n",
        "    return dict([(palavra, palavras.count(palavra)) for palavra in palavras])\n",
        "    \n",
        "\n",
        "def treina_classificadores(neutro,positivo,negativo): \n",
        "    posdados = []\n",
        "    with open(positivo, 'r') as myfile:   \n",
        "        reader = csv.reader(myfile, delimiter=',') \n",
        "        for val in reader:\n",
        "            if len(val) > 0:\n",
        "                posdados.append(val[0]) \n",
        "\n",
        "    negdados = [] \n",
        "    with open(negativo, 'r') as myfile:    \n",
        "        reader = csv.reader(myfile, delimiter=',')\n",
        "        for val in reader:\n",
        "            if len(val) > 0:\n",
        "                negdados.append(val[0])    \n",
        "\n",
        "    neudados = [] \n",
        "    with open(neutro, 'r') as myfile:    \n",
        "        reader = csv.reader(myfile, delimiter=',')\n",
        "        for val in reader:\n",
        "            if len(val) > 0:\n",
        "                neudados.append(val[0])                  \n",
        "\n",
        "    negfeats = [(bag_of_words(f), 'neg') for f in divide(negdados)] #pra cada tweet eu coloquei o marcado neg\n",
        "    posfeats = [(bag_of_words(f), 'pos') for f in divide(posdados)] #pra cada tweet eu coloquei o marcado pos\n",
        "    neufeats = [(bag_of_words(f), 'neu') for f in divide(neudados)] #pra cada tweet eu coloquei o marcado neu\n",
        "   \n",
        "    treino = negfeats + posfeats + neufeats\n",
        "\n",
        "    \n",
        "    classificadorME = MaxentClassifier.train(treino, 'GIS', trace=0, encoding=None, labels=None, gaussian_prior_sigma=0, max_iter = 1)\n",
        "    classificadorSVM = SklearnClassifier(LinearSVC(), sparse=False)\n",
        "    classificadorSVM.train(treino)\n",
        "    classificadorNB = NaiveBayesClassifier.train(treino)\n",
        "    \n",
        "    return ([classificadorME,classificadorSVM,classificadorNB])\n",
        "\n",
        "\n",
        "#aqui eu vou chamar a classifica√ß√£o de tweets novos        \n",
        "#eu recebo uma lista de tweets e os objetos dos classificadores que eu treinei anteriormento\n",
        "def classifica(sentencas, classificadores):\n",
        "    ret = []                        \n",
        "    for s in sentencas: #pra cada um dos tweets\n",
        "        c = divide([s])\n",
        "        feats= bag_of_words(c[0])\n",
        "        classificacao = []\n",
        "        classificacao.append(classificadores[1].classify(feats))\n",
        "        classificacao.append(classificadores[2].classify(feats))\n",
        "        classificacao.append(classificadores[0].classify(feats))\n",
        "        ret.append(classificacao)\n",
        "    return ret\n"
      ],
      "id": "n6J-GkO0X4SL",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75afb1cc",
        "outputId": "d4ffc4f7-0b97-4e6a-c6b9-124376abc13f"
      },
      "source": [
        "######## MAIN             \n",
        "neutro='tweets_neutro.txt'\n",
        "positivo='tweets_positivo.txt'\n",
        "negativo='tweets_negativo.txt'\n",
        "\n",
        "\n",
        "classificadores = treina_classificadores(neutro,positivo,negativo)\n",
        "\n",
        "\n",
        "sentences = ['Eu odeio o Sergio', 'espero que a Debi seja eliminada', 'os pratos est√£o com uma cara √≥tima!']\n",
        "\n",
        "print (classifica(sentences, classificadores))\n",
        "\n",
        "\n",
        "  \n",
        "\n"
      ],
      "id": "75afb1cc",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['neg', 'pos', 'pos'], ['pos', 'pos', 'pos'], ['neu', 'neg', 'neg']]\n"
          ]
        }
      ]
    }
  ]
}