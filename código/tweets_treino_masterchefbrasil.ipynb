{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "tweets_treino_masterchefbrasil.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNQgjhujZ4Ez"
      },
      "source": [
        "# **Trabalho Prático 1: Atividade em conjunto de classificação de texto**\n",
        "### Grupo: Bianca Muniz, Renan Cavalcante, Sara Magalhães e Talita Burbulhan\n",
        "### Tema: Masterchef Brasil\n",
        "**Objetivo**: Efetuar a classificação do corpus capturado na primeira aula e classes determinadas pelos grupos\n",
        "\n",
        "**Passos**\n",
        "* Criar corpus de treino\n",
        "* Rodar algoritmos de treino e classificação\n",
        "* Enviar 1 link para notebook no google colab por grupo"
      ],
      "id": "qNQgjhujZ4Ez"
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "93328deb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fd68dd4-e617-4111-b043-939003eb5725"
      },
      "source": [
        "# Instalando e importando bibliotecas\n",
        "!pip3 install snscrape\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "import pandas as pd"
      ],
      "id": "93328deb",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting snscrape\n",
            "  Downloading snscrape-0.3.4-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from snscrape) (4.2.6)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from snscrape) (4.6.3)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from snscrape) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->snscrape) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->snscrape) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->snscrape) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->snscrape) (2021.5.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->snscrape) (1.7.1)\n",
            "Installing collected packages: snscrape\n",
            "Successfully installed snscrape-0.3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b4937e8",
        "scrolled": false
      },
      "source": [
        "# Criando uma lista para coletar os tweets com a query - biblioteca snscrape\n",
        "tweets = []\n",
        "# Pegando mil tweets em português da hashtag #masterchefbrasil\n",
        "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('#masterchefbrasil lang:pt -filter:replies').get_items()):\n",
        "    if i>1000:\n",
        "        break\n",
        "    tweets.append([tweet.id, tweet.date, tweet.content, tweet.username])\n",
        "\n",
        "masterchef = pd.DataFrame(tweets)\n",
        "masterchef.to_csv(\"masterchef_treino.csv\", sep='\\t', encoding='utf-8', index=False)"
      ],
      "id": "5b4937e8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXmjcrlqVv2g"
      },
      "source": [
        "Importamos o arquivo CSV para o Google Drive e classificamos os tweets manualmente em três grupos (neutro, positivo ou negativo), até cada grupo ter 210 tweets. A planilha pode ser vista no [link](https://github.com/biamuniz/mineracao_masterchefbrasil/blob/main/tweets_treino.csv)."
      ],
      "id": "nXmjcrlqVv2g"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taRkLgi5Vf0h"
      },
      "source": [
        "#Abrindo o arquivo CSV com os tweets classificados\n",
        "dados = pd.read_csv(r'https://raw.githubusercontent.com/biamuniz/mineracao_masterchefbrasil/main/dados/tweets_treino.csv', sep=',')"
      ],
      "id": "taRkLgi5Vf0h",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "zLddg7TNXJdj",
        "outputId": "a90684ce-2549-4bb7-dfe8-2b3dc507d75c"
      },
      "source": [
        "dados"
      ],
      "id": "zLddg7TNXJdj",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>data</th>\n",
              "      <th>tweet</th>\n",
              "      <th>user</th>\n",
              "      <th>classificação</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1456834542627602433</td>\n",
              "      <td>2021-11-06 04:03:16+00:00</td>\n",
              "      <td>O que aconteceu com o Fogaça? Vendo agora o ep...</td>\n",
              "      <td>sferreiravic</td>\n",
              "      <td>Neutro</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1456765784529768458</td>\n",
              "      <td>2021-11-05 23:30:03+00:00</td>\n",
              "      <td>Em 2020, Marília participou do episódio especi...</td>\n",
              "      <td>RevistaISTOE</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1456764222977761284</td>\n",
              "      <td>2021-11-05 23:23:50+00:00</td>\n",
              "      <td>Maria Rita e João Carlos Martins são os convid...</td>\n",
              "      <td>tvabordo</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1456442462743015452</td>\n",
              "      <td>2021-11-05 02:05:17+00:00</td>\n",
              "      <td>Eu não entendo a perseguição com o Heitor e o ...</td>\n",
              "      <td>sabilisacrf</td>\n",
              "      <td>Positivo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1456316523019128840</td>\n",
              "      <td>2021-11-04 17:44:50+00:00</td>\n",
              "      <td>O horário de trabalho de todo mundo que trabal...</td>\n",
              "      <td>chefanahirata</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>1418040353115066378</td>\n",
              "      <td>2021-07-22 02:49:00+00:00</td>\n",
              "      <td>Gente…faço nem ideia de quem seja essa duda be...</td>\n",
              "      <td>nnalourenco</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>1418040340305620999</td>\n",
              "      <td>2021-07-22 02:48:57+00:00</td>\n",
              "      <td>Eu comecei não gostando da Amanda e agora ela ...</td>\n",
              "      <td>ccolemanholland</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>1418040289365827592</td>\n",
              "      <td>2021-07-22 02:48:45+00:00</td>\n",
              "      <td>Alguém sabe quem é Duda Beat?\\n#masterchefbrasil</td>\n",
              "      <td>LuizMelloc</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>1418040242339209218</td>\n",
              "      <td>2021-07-22 02:48:34+00:00</td>\n",
              "      <td>Duda Beat no #masterchefbrasil é tudo pra mim 🥰🥰</td>\n",
              "      <td>lholhohelo</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1000</th>\n",
              "      <td>1418040232692404229</td>\n",
              "      <td>2021-07-22 02:48:32+00:00</td>\n",
              "      <td>Essa Juliana tem uma cara de enjoada... \\n#mas...</td>\n",
              "      <td>ravennacomcats</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1001 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                       id  ... classificação\n",
              "0     1456834542627602433  ...        Neutro\n",
              "1     1456765784529768458  ...           NaN\n",
              "2     1456764222977761284  ...           NaN\n",
              "3     1456442462743015452  ...      Positivo\n",
              "4     1456316523019128840  ...           NaN\n",
              "...                   ...  ...           ...\n",
              "996   1418040353115066378  ...           NaN\n",
              "997   1418040340305620999  ...           NaN\n",
              "998   1418040289365827592  ...           NaN\n",
              "999   1418040242339209218  ...           NaN\n",
              "1000  1418040232692404229  ...           NaN\n",
              "\n",
              "[1001 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjNk58FhaUZg"
      },
      "source": [
        "### **Filtrando os tweets classificados e salvando cada grupo em um arquivo de texto diferente**"
      ],
      "id": "VjNk58FhaUZg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6P3FSBGXN82"
      },
      "source": [
        "Tweets negativos"
      ],
      "id": "e6P3FSBGXN82"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIlaijnOVoAI"
      },
      "source": [
        "negativo = dados.loc[dados['classificação'] == 'Negativo'] ## filtrando os tweets com classificação \"Negativo\" e armazenando no objeto \"negativo\"\n",
        "tweet_negativo = negativo.drop(columns=['id', \"data\", \"user\", \"classificação\"]) ##excluindo colunas e deixando só a coluna de interesse (tweet)\n",
        "tweet_negativo.to_csv(\"tweets_negativo.txt\", sep='\\t', encoding='utf-8', index = False) ## salvando o objeto em txt com o nome tweets_negativo.txt"
      ],
      "id": "YIlaijnOVoAI",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_xX0fqQXpB8"
      },
      "source": [
        "Tweets positivos"
      ],
      "id": "g_xX0fqQXpB8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jz5id5ZXXW4P"
      },
      "source": [
        "positivo = dados.loc[dados['classificação'] == 'Positivo'] ## filtrando os tweets com classificação \"Positivo\" e armazenando no objeto \"negativo\"\n",
        "tweet_positivo = positivo.drop(columns=['id', \"data\", \"user\", \"classificação\"]) ##excluindo colunas e deixando só a coluna de interesse (tweet)\n",
        "tweet_positivo.to_csv(\"tweets_positivo.txt\", sep='\\t', encoding='utf-8', index = False) ## salvando o objeto em txt com o nome tweets_negativo.txt"
      ],
      "id": "Jz5id5ZXXW4P",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgYVHoznXrDB"
      },
      "source": [
        "Tweets neutros"
      ],
      "id": "QgYVHoznXrDB"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVOs7vykXWpk"
      },
      "source": [
        "neutro = dados.loc[dados['classificação'] == 'Neutro'] ## filtrando os tweets com classificação \"Negativo\" e armazenando no objeto \"negativo\"\n",
        "tweet_neutro = neutro.drop(columns=['id', \"data\", \"user\", \"classificação\"]) ##excluindo colunas e deixando só a coluna de interesse (tweet)\n",
        "tweet_neutro.to_csv(\"tweets_neutro.txt\", sep='\\t', encoding='utf-8', index = False) ## salvando o objeto em txt com o nome tweets_negativo.txt"
      ],
      "id": "OVOs7vykXWpk",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOeAH7pIaiqa"
      },
      "source": [
        "### **Rodando o modelo**"
      ],
      "id": "KOeAH7pIaiqa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7CvyVIFXxw5",
        "outputId": "56651ddc-4704-485d-f09b-9ca8ef3c806b"
      },
      "source": [
        "!pip install nltk"
      ],
      "id": "I7CvyVIFXxw5",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6J-GkO0X4SL"
      },
      "source": [
        "from nltk.classify import NaiveBayesClassifier, MaxentClassifier, SklearnClassifier\n",
        "import csv\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "def divide(dados):    \n",
        "    dados_new = []\n",
        "    for palavra in dados:\n",
        "        palavra_filter = [i.lower() for i in palavra.split()]\n",
        "        dados_new.append(palavra_filter)\n",
        "    return dados_new\n",
        "\n",
        "def bag_of_words(palavras):\n",
        "    return dict([(palavra, palavras.count(palavra)) for palavra in palavras])\n",
        "    \n",
        "\n",
        "def treina_classificadores(neutro,positivo,negativo): \n",
        "    posdados = []\n",
        "    with open(positivo, 'r') as myfile:   \n",
        "        reader = csv.reader(myfile, delimiter=',') \n",
        "        for val in reader:\n",
        "            if len(val) > 0:\n",
        "                posdados.append(val[0]) \n",
        "\n",
        "    negdados = [] \n",
        "    with open(negativo, 'r') as myfile:    \n",
        "        reader = csv.reader(myfile, delimiter=',')\n",
        "        for val in reader:\n",
        "            if len(val) > 0:\n",
        "                negdados.append(val[0])    \n",
        "\n",
        "    neudados = [] \n",
        "    with open(neutro, 'r') as myfile:    \n",
        "        reader = csv.reader(myfile, delimiter=',')\n",
        "        for val in reader:\n",
        "            if len(val) > 0:\n",
        "                neudados.append(val[0])                  \n",
        "\n",
        "    negfeats = [(bag_of_words(f), 'neg') for f in divide(negdados)] #pra cada tweet eu coloquei o marcado neg\n",
        "    posfeats = [(bag_of_words(f), 'pos') for f in divide(posdados)] #pra cada tweet eu coloquei o marcado pos\n",
        "    neufeats = [(bag_of_words(f), 'neu') for f in divide(neudados)] #pra cada tweet eu coloquei o marcado neu\n",
        "   \n",
        "    treino = negfeats + posfeats + neufeats\n",
        "\n",
        "    \n",
        "    classificadorME = MaxentClassifier.train(treino, 'GIS', trace=0, encoding=None, labels=None, gaussian_prior_sigma=0, max_iter = 1)\n",
        "    classificadorSVM = SklearnClassifier(LinearSVC(), sparse=False)\n",
        "    classificadorSVM.train(treino)\n",
        "    classificadorNB = NaiveBayesClassifier.train(treino)\n",
        "    \n",
        "    return ([classificadorME,classificadorSVM,classificadorNB])\n",
        "\n",
        "\n",
        "#aqui eu vou chamar a classificação de tweets novos        \n",
        "#eu recebo uma lista de tweets e os objetos dos classificadores que eu treinei anteriormento\n",
        "def classifica(sentencas, classificadores):\n",
        "    ret = []                        \n",
        "    for s in sentencas: #pra cada um dos tweets\n",
        "        c = divide([s])\n",
        "        feats= bag_of_words(c[0])\n",
        "        classificacao = []\n",
        "        classificacao.append(classificadores[1].classify(feats))\n",
        "        classificacao.append(classificadores[2].classify(feats))\n",
        "        classificacao.append(classificadores[0].classify(feats))\n",
        "        ret.append(classificacao)\n",
        "    return ret\n"
      ],
      "id": "n6J-GkO0X4SL",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75afb1cc",
        "outputId": "d4ffc4f7-0b97-4e6a-c6b9-124376abc13f"
      },
      "source": [
        "######## MAIN             \n",
        "neutro='tweets_neutro.txt'\n",
        "positivo='tweets_positivo.txt'\n",
        "negativo='tweets_negativo.txt'\n",
        "\n",
        "\n",
        "classificadores = treina_classificadores(neutro,positivo,negativo)\n",
        "\n",
        "\n",
        "sentences = ['Eu odeio o Sergio', 'espero que a Debi seja eliminada', 'os pratos estão com uma cara ótima!']\n",
        "\n",
        "print (classifica(sentences, classificadores))\n",
        "\n",
        "\n",
        "  \n",
        "\n"
      ],
      "id": "75afb1cc",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['neg', 'pos', 'pos'], ['pos', 'pos', 'pos'], ['neu', 'neg', 'neg']]\n"
          ]
        }
      ]
    }
  ]
}